{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Exercise\n",
    "\n",
    "In this notebook, we will look Yelp review data. It has 37938 rows of review data, and each row has some data about the restaurant as well as its star rating. The object of our model will be predict the star rating given the other data. Much of the information is categorical, NOT numeric nor continuous, so we perform one-hot encoding on them.\n",
    "\n",
    "The data is split into train and test for you already. Metric we will use is mean squared error (MSE).\n",
    "The minimum linear regression model is implemented below, but it obviously performs very poorly.\n",
    "\n",
    "Your task is to implement a better model that will perform better. It must have MSE below 1.0. Lowest score can have bragging rights :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    data = pd.DataFrame([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract x & y, then split into train & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_ratings = data['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.get_dummies(data.city, drop_first=True)\n",
    "categories = pd.get_dummies(data.categories.apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "features = pd.concat([cities, categories, data.latitude, data.longitude], axis=1).fillna(0.)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, star_ratings, test_size=0.2, random_state=710)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define my model and fit it to the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the test dataset and calculate the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "Change & tune the model so that you get MSE at least below 1.0). Can you get below 0.66?\n",
    "\n",
    "It might be useful to look at what models are available [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model).  \n",
    "[This](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) might be useful too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
